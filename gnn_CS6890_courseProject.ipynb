{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4LBFkPZN72B"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit\n",
        "!pip install mlflow\n",
        "!pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install --pre deepchem\n",
        "!pip install arm-mango"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Dataset, Data\n",
        "import numpy as np \n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
        "print(f\"Torch geometric version: {torch_geometric.__version__}\")\n",
        "\n",
        "\"\"\"\n",
        "!!!\n",
        "NOTE: This file was replaced by dataset_featurizer.py\n",
        "but is kept to illustrate how to build a custom dataset in PyG.\n",
        "!!!\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class MoleculeDataset(Dataset):\n",
        "    def __init__(self, root, filename, test=False, transform=None, pre_transform=None):\n",
        "        \"\"\"\n",
        "        root = Where the dataset should be stored. This folder is split\n",
        "        into raw_dir (downloaded dataset) and processed_dir (processed data). \n",
        "        \"\"\"\n",
        "        self.test = test\n",
        "        self.filename = filename\n",
        "        super(MoleculeDataset, self).__init__(root, transform, pre_transform)\n",
        "        \n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        \"\"\" If this file exists in raw_dir, the download is not triggered.\n",
        "            (The download func. is not implemented here)  \n",
        "        \"\"\"\n",
        "        return self.filename\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        \"\"\" If these files are found in raw_dir, processing is skipped\"\"\"\n",
        "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
        "\n",
        "        if self.test:\n",
        "            return [f'data_test_{i}.pt' for i in list(self.data.index)]\n",
        "        else:\n",
        "            return [f'data_{i}.pt' for i in list(self.data.index)]\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        self.data = pd.read_csv(self.raw_paths[0])\n",
        "        for index, mol in tqdm(self.data.iterrows(), total=self.data.shape[0]):\n",
        "            mol_obj = Chem.MolFromSmiles(mol[\"smiles\"])\n",
        "            # Get node features\n",
        "            node_feats = self._get_node_features(mol_obj)\n",
        "            # Get edge features\n",
        "            edge_feats = self._get_edge_features(mol_obj)\n",
        "            # Get adjacency info\n",
        "            edge_index = self._get_adjacency_info(mol_obj)\n",
        "            # Get labels info\n",
        "            label = self._get_labels(mol[\"HIV_active\"])\n",
        "\n",
        "            # Create data object\n",
        "            data = Data(x=node_feats, \n",
        "                        edge_index=edge_index,\n",
        "                        edge_attr=edge_feats,\n",
        "                        y=label,\n",
        "                        smiles=mol[\"smiles\"]\n",
        "                        ) \n",
        "            if self.test:\n",
        "                torch.save(data, \n",
        "                    os.path.join(self.processed_dir, \n",
        "                                 f'data_test_{index}.pt'))\n",
        "            else:\n",
        "                torch.save(data, \n",
        "                    os.path.join(self.processed_dir, \n",
        "                                 f'data_{index}.pt'))\n",
        "\n",
        "    def _get_node_features(self, mol):\n",
        "        \"\"\" \n",
        "        This will return a matrix / 2d array of the shape\n",
        "        [Number of Nodes, Node Feature size]\n",
        "        \"\"\"\n",
        "        all_node_feats = []\n",
        "\n",
        "        for atom in mol.GetAtoms():\n",
        "            node_feats = []\n",
        "            # Feature 1: Atomic number        \n",
        "            node_feats.append(atom.GetAtomicNum())\n",
        "            # Feature 2: Atom degree\n",
        "            node_feats.append(atom.GetDegree())\n",
        "            # Feature 3: Formal charge\n",
        "            node_feats.append(atom.GetFormalCharge())\n",
        "            # Feature 4: Hybridization\n",
        "            node_feats.append(atom.GetHybridization())\n",
        "            # Feature 5: Aromaticity\n",
        "            node_feats.append(atom.GetIsAromatic())\n",
        "            # Feature 6: Total Num Hs\n",
        "            node_feats.append(atom.GetTotalNumHs())\n",
        "            # Feature 7: Radical Electrons\n",
        "            node_feats.append(atom.GetNumRadicalElectrons())\n",
        "            # Feature 8: In Ring\n",
        "            node_feats.append(atom.IsInRing())\n",
        "            # Feature 9: Chirality\n",
        "            node_feats.append(atom.GetChiralTag())\n",
        "\n",
        "            # Append node features to matrix\n",
        "            all_node_feats.append(node_feats)\n",
        "\n",
        "        all_node_feats = np.asarray(all_node_feats)\n",
        "        return torch.tensor(all_node_feats, dtype=torch.float)\n",
        "\n",
        "    def _get_edge_features(self, mol):\n",
        "        \"\"\" \n",
        "        This will return a matrix / 2d array of the shape\n",
        "        [Number of edges, Edge Feature size]\n",
        "        \"\"\"\n",
        "        all_edge_feats = []\n",
        "\n",
        "        for bond in mol.GetBonds():\n",
        "            edge_feats = []\n",
        "            # Feature 1: Bond type (as double)\n",
        "            edge_feats.append(bond.GetBondTypeAsDouble())\n",
        "            # Feature 2: Rings\n",
        "            edge_feats.append(bond.IsInRing())\n",
        "            # Append node features to matrix (twice, per direction)\n",
        "            all_edge_feats += [edge_feats, edge_feats]\n",
        "\n",
        "        all_edge_feats = np.asarray(all_edge_feats)\n",
        "        return torch.tensor(all_edge_feats, dtype=torch.float)\n",
        "\n",
        "    def _get_adjacency_info(self, mol):\n",
        "        \"\"\"\n",
        "        We could also use rdmolops.GetAdjacencyMatrix(mol)\n",
        "        but we want to be sure that the order of the indices\n",
        "        matches the order of the edge features\n",
        "        \"\"\"\n",
        "        edge_indices = []\n",
        "        for bond in mol.GetBonds():\n",
        "            i = bond.GetBeginAtomIdx()\n",
        "            j = bond.GetEndAtomIdx()\n",
        "            edge_indices += [[i, j], [j, i]]\n",
        "\n",
        "        edge_indices = torch.tensor(edge_indices)\n",
        "        edge_indices = edge_indices.t().to(torch.long).view(2, -1)\n",
        "        return edge_indices\n",
        "\n",
        "    def _get_labels(self, label):\n",
        "        label = np.asarray([label])\n",
        "        return torch.tensor(label, dtype=torch.int64)\n",
        "\n",
        "    def len(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def get(self, idx):\n",
        "        \"\"\" - Equivalent to __getitem__ in pytorch\n",
        "            - Is not needed for PyG's InMemoryDataset\n",
        "        \"\"\"\n",
        "        if self.test:\n",
        "            data = torch.load(os.path.join(self.processed_dir, \n",
        "                                 f'data_test_{idx}.pt'))\n",
        "        else:\n",
        "            data = torch.load(os.path.join(self.processed_dir, \n",
        "                                 f'data_{idx}.pt'))   \n",
        "        return data"
      ],
      "metadata": {
        "id": "k5QGZT-L28Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import numpy as np\n",
        "from mlflow.models.signature import ModelSignature\n",
        "from mlflow.types.schema import Schema, TensorSpec\n",
        "\n",
        "\n",
        "HYPERPARAMETERS = {\n",
        "    \"batch_size\": [32, 128, 64],\n",
        "    \"learning_rate\": [0.1, 0.05, 0.01, 0.001],\n",
        "    \"weight_decay\": [0.0001, 0.00001, 0.001],\n",
        "    \"sgd_momentum\": [0.9, 0.8, 0.5],\n",
        "    \"scheduler_gamma\": [0.995, 0.9, 0.8, 0.5, 1],\n",
        "    \"pos_weight\" : [1.0,1.3,0.8,0.9],  \n",
        "    \"model_embedding_size\": [8, 16, 32, 64, 128],\n",
        "    \"model_attention_heads\": [1, 2, 3, 4],\n",
        "    \"model_layers\": [2,4,5,8,10],\n",
        "    \"model_dropout_rate\": [0.2, 0.5, 0.9],\n",
        "    \"model_top_k_ratio\": [0.2, 0.5, 0.8, 0.9],\n",
        "    \"model_top_k_every_n\": [1,2,3],\n",
        "    \"model_dense_neurons\": [16, 128, 64, 256, 32]\n",
        "}\n",
        "\n",
        "BEST_PARAMETERS = {\n",
        "    \"batch_size\": [128],\n",
        "    \"learning_rate\": [0.01],\n",
        "    \"weight_decay\": [0.0001],\n",
        "    \"sgd_momentum\": [0.8],\n",
        "    \"scheduler_gamma\": [0.8],\n",
        "    \"pos_weight\": [1.3],\n",
        "    \"model_embedding_size\": [64],\n",
        "    \"model_attention_heads\": [3],\n",
        "    \"model_layers\": [4],\n",
        "    \"model_dropout_rate\": [0.2],\n",
        "    \"model_top_k_ratio\": [0.5],\n",
        "    \"model_top_k_every_n\": [1],\n",
        "    \"model_dense_neurons\": [256]\n",
        "}\n",
        "\n",
        "input_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 30), name=\"x\"), \n",
        "                       TensorSpec(np.dtype(np.float32), (-1, 11), name=\"edge_attr\"), \n",
        "                       TensorSpec(np.dtype(np.int32), (2, -1), name=\"edge_index\"), \n",
        "                       TensorSpec(np.dtype(np.int32), (-1, 1), name=\"batch_index\")])\n",
        "\n",
        "output_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 1))])\n",
        "\n",
        "SIGNATURE = ModelSignature(inputs=input_schema, outputs=output_schema)"
      ],
      "metadata": {
        "id": "2V9uH05ZQSsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F \n",
        "from torch.nn import Linear, BatchNorm1d, ModuleList\n",
        "from torch_geometric.nn import TransformerConv, TopKPooling \n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, feature_size, model_params):\n",
        "        super(GNN, self).__init__()\n",
        "        embedding_size = model_params[\"model_embedding_size\"]\n",
        "        n_heads = model_params[\"model_attention_heads\"]\n",
        "        self.n_layers = model_params[\"model_layers\"]\n",
        "        dropout_rate = model_params[\"model_dropout_rate\"]\n",
        "        top_k_ratio = model_params[\"model_top_k_ratio\"]\n",
        "        self.top_k_every_n = model_params[\"model_top_k_every_n\"]\n",
        "        dense_neurons = model_params[\"model_dense_neurons\"]\n",
        "        edge_dim = model_params[\"model_edge_dim\"]\n",
        "\n",
        "        self.conv_layers = ModuleList([])\n",
        "        self.transf_layers = ModuleList([])\n",
        "        self.pooling_layers = ModuleList([])\n",
        "        self.bn_layers = ModuleList([])\n",
        "\n",
        "        # Transformation layer\n",
        "        self.conv1 = TransformerConv(feature_size, \n",
        "                                    embedding_size, \n",
        "                                    heads=n_heads, \n",
        "                                    dropout=dropout_rate,\n",
        "                                    edge_dim=edge_dim,\n",
        "                                    beta=True) \n",
        "\n",
        "        self.transf1 = Linear(embedding_size*n_heads, embedding_size)\n",
        "        self.bn1 = BatchNorm1d(embedding_size)\n",
        "\n",
        "        # Other layers\n",
        "        for i in range(self.n_layers):\n",
        "            self.conv_layers.append(TransformerConv(embedding_size, \n",
        "                                                    embedding_size, \n",
        "                                                    heads=n_heads, \n",
        "                                                    dropout=dropout_rate,\n",
        "                                                    edge_dim=edge_dim,\n",
        "                                                    beta=True))\n",
        "\n",
        "            self.transf_layers.append(Linear(embedding_size*n_heads, embedding_size))\n",
        "            self.bn_layers.append(BatchNorm1d(embedding_size))\n",
        "            if i % self.top_k_every_n == 0:\n",
        "                self.pooling_layers.append(TopKPooling(embedding_size, ratio=top_k_ratio))\n",
        "            \n",
        "\n",
        "        # Linear layers\n",
        "        self.linear1 = Linear(embedding_size*2, dense_neurons)\n",
        "        self.linear2 = Linear(dense_neurons, int(dense_neurons/2))  \n",
        "        self.linear3 = Linear(int(dense_neurons/2), 1)  \n",
        "\n",
        "    def forward(self, x, edge_attr, edge_index, batch_index):\n",
        "        # Initial transformation\n",
        "        x = self.conv1(x, edge_index, edge_attr)\n",
        "        x = torch.relu(self.transf1(x))\n",
        "        x = self.bn1(x)\n",
        "\n",
        "        # Holds the intermediate graph representations\n",
        "        global_representation = []\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.conv_layers[i](x, edge_index, edge_attr)\n",
        "            x = torch.relu(self.transf_layers[i](x))\n",
        "            x = self.bn_layers[i](x)\n",
        "            # Always aggregate last layer\n",
        "            if i % self.top_k_every_n == 0 or i == self.n_layers:\n",
        "                x , edge_index, edge_attr, batch_index, _, _ = self.pooling_layers[int(i/self.top_k_every_n)](\n",
        "                    x, edge_index, edge_attr, batch_index\n",
        "                    )\n",
        "                # Add current representation\n",
        "                global_representation.append(torch.cat([gmp(x, batch_index), gap(x, batch_index)], dim=1))\n",
        "    \n",
        "        x = sum(global_representation)\n",
        "\n",
        "        # Output block\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        x = F.dropout(x, p=0.8, training=self.training)\n",
        "        x = torch.relu(self.linear2(x))\n",
        "        x = F.dropout(x, p=0.8, training=self.training)\n",
        "        x = self.linear3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "qX3wqMFJOUGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "from torch_geometric.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, f1_score, \\\n",
        "    accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import pandas as pd \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def train_one_epoch(epoch, model, train_loader, optimizer, loss_fn):\n",
        "    # Enumerate over the data\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    running_loss = 0.0\n",
        "    step = 0\n",
        "    for _, batch in enumerate(tqdm(train_loader)):\n",
        "        # Use GPU\n",
        "        batch.to(device)  \n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad() \n",
        "        # Passing the node features and the connection info\n",
        "        pred = model(batch.x.float(), \n",
        "                                batch.edge_attr.float(),\n",
        "                                batch.edge_index, \n",
        "                                batch.batch) \n",
        "        # Calculating the loss and gradients\n",
        "        loss = loss_fn(torch.squeeze(pred), batch.y.float())\n",
        "        loss.backward()  \n",
        "        optimizer.step()  \n",
        "        # Update tracking\n",
        "        running_loss += loss.item()\n",
        "        step += 1\n",
        "        all_preds.append(np.rint(torch.sigmoid(pred).cpu().detach().numpy()))\n",
        "        all_labels.append(batch.y.cpu().detach().numpy())\n",
        "    all_preds = np.concatenate(all_preds).ravel()\n",
        "    all_labels = np.concatenate(all_labels).ravel()\n",
        "    calculate_metrics(all_preds, all_labels, epoch, \"train\")\n",
        "    return running_loss/step\n",
        "\n",
        "def test(epoch, model, test_loader, loss_fn):\n",
        "    all_preds = []\n",
        "    all_preds_raw = []\n",
        "    all_labels = []\n",
        "    running_loss = 0.0\n",
        "    step = 0\n",
        "    for batch in test_loader:\n",
        "        batch.to(device)  \n",
        "        pred = model(batch.x.float(), \n",
        "                        batch.edge_attr.float(),\n",
        "                        batch.edge_index, \n",
        "                        batch.batch) \n",
        "        loss = loss_fn(torch.squeeze(pred), batch.y.float())\n",
        "\n",
        "         # Update tracking\n",
        "        running_loss += loss.item()\n",
        "        step += 1\n",
        "        all_preds.append(np.rint(torch.sigmoid(pred).cpu().detach().numpy()))\n",
        "        all_preds_raw.append(torch.sigmoid(pred).cpu().detach().numpy())\n",
        "        all_labels.append(batch.y.cpu().detach().numpy())\n",
        "    \n",
        "    all_preds = np.concatenate(all_preds).ravel()\n",
        "    all_labels = np.concatenate(all_labels).ravel()\n",
        "    print(all_preds_raw[0][:10])\n",
        "    print(all_preds[:10])\n",
        "    print(all_labels[:10])\n",
        "    calculate_metrics(all_preds, all_labels, epoch, \"test\")\n",
        "    log_conf_matrix(all_preds, all_labels, epoch)\n",
        "    return running_loss/step\n",
        "\n",
        "def log_conf_matrix(y_pred, y_true, epoch):\n",
        "    # Log confusion matrix as image\n",
        "    cm = confusion_matrix(y_pred, y_true)\n",
        "    classes = [\"0\", \"1\"]\n",
        "    df_cfm = pd.DataFrame(cm, index = classes, columns = classes)\n",
        "    plt.figure(figsize = (10,7))\n",
        "    cfm_plot = sns.heatmap(df_cfm, annot=True, cmap='Blues', fmt='g')\n",
        "    cfm_plot.figure.savefig(f'data/images/cm_{epoch}.png')\n",
        "    mlflow.log_artifact(f\"data/images/cm_{epoch}.png\")\n",
        "\n",
        "def calculate_metrics(y_pred, y_true, epoch, type):\n",
        "    print(f\"\\n Confusion matrix: \\n {confusion_matrix(y_pred, y_true)}\")\n",
        "    print(f\"F1 Score: {f1_score(y_true, y_pred)}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
        "    prec = precision_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    print(f\"Precision: {prec}\")\n",
        "    print(f\"Recall: {rec}\")\n",
        "    mlflow.log_metric(key=f\"Precision-{type}\", value=float(prec), step=epoch)\n",
        "    mlflow.log_metric(key=f\"Recall-{type}\", value=float(rec), step=epoch)\n",
        "    try:\n",
        "        roc = roc_auc_score(y_true, y_pred)\n",
        "        print(f\"ROC AUC: {roc}\")\n",
        "        mlflow.log_metric(key=f\"ROC-AUC-{type}\", value=float(roc), step=epoch)\n",
        "    except:\n",
        "        mlflow.log_metric(key=f\"ROC-AUC-{type}\", value=float(0), step=epoch)\n",
        "        print(f\"ROC AUC: notdefined\")\n",
        "\n",
        "# %% Run the training\n",
        "from mango import scheduler, Tuner\n",
        "\n",
        "\n",
        "def run_one_training(params):\n",
        "  params = params[0]\n",
        "  with mlflow.start_run() as run:\n",
        "    for key in params.keys():\n",
        "      mlflow.log_param(key, params[key])\n",
        "      print(\"Loading dataset...\")\n",
        "      train_dataset = MoleculeDataset(root=\"data/\", filename=\"HIV_train_oversampled.csv\")\n",
        "      test_dataset = MoleculeDataset(root=\"data/\", filename=\"HIV_test.csv\", test=True)\n",
        "      params[\"model_edge_dim\"] = train_dataset[0].edge_attr.shape[1]\n",
        "\n",
        "        # Prepare training\n",
        "      train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
        "      test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
        "\n",
        "        # Loading the model\n",
        "      print(\"Loading model...\")\n",
        "      model_params = {k: v for k, v in params.items() if k.startswith(\"model_\")}\n",
        "      model = GNN(feature_size=train_dataset[0].x.shape[1], model_params=model_params) \n",
        "      model = model.to(device)\n",
        "      print(f\"Number of parameters: {count_parameters(model)}\")\n",
        "      mlflow.log_param(\"num_params\", count_parameters(model))\n",
        "\n",
        "        # < 1 increases precision, > 1 recall\n",
        "      weight = torch.tensor([params[\"pos_weight\"]], dtype=torch.float32).to(device)\n",
        "      loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=weight)\n",
        "      optimizer = torch.optim.SGD(model.parameters(), \n",
        "                                    lr=params[\"learning_rate\"],\n",
        "                                    momentum=params[\"sgd_momentum\"],\n",
        "                                    weight_decay=params[\"weight_decay\"])\n",
        "      scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=params[\"scheduler_gamma\"])\n",
        "        \n",
        "        # Start training\n",
        "      best_loss = 1000\n",
        "      early_stopping_counter = 0\n",
        "      for epoch in range(50): \n",
        "          if early_stopping_counter <= 10:\n",
        "            model.train()\n",
        "            loss = train_one_epoch(epoch, model, train_loader, optimizer, loss_fn)\n",
        "            print(f\"Epoch {epoch} | Train Loss {loss}\")\n",
        "            mlflow.log_metric(key=\"Train loss\", value=float(loss), step=epoch)\n",
        "\n",
        "                # Testing\n",
        "            model.eval()\n",
        "            if epoch % 5 == 0:\n",
        "                loss = test(epoch, model, test_loader, loss_fn)\n",
        "                print(f\"Epoch {epoch} | Test Loss {loss}\")\n",
        "                mlflow.log_metric(key=\"Test loss\", value=float(loss), step=epoch)\n",
        "                    \n",
        "                    # Update best loss\n",
        "                if float(loss) < best_loss:\n",
        "                    best_loss = loss\n",
        "                        # Save the currently best model \n",
        "                    mlflow.pytorch.log_model(model, \"model\", signature=SIGNATURE)\n",
        "                    early_stopping_counter = 0\n",
        "                else:\n",
        "                    early_stopping_counter += 1\n",
        "\n",
        "            scheduler.step()\n",
        "          else:\n",
        "              print(\"Early stopping due to no improvement.\")\n",
        "              return [best_loss]\n",
        "  print(f\"Finishing training with best test loss: {best_loss}\")\n",
        "  return [best_loss]\n",
        "\n",
        "# %% Hyperparameter search\n",
        "print(\"Running hyperparameter search...\")\n",
        "config = dict()\n",
        "config[\"optimizer\"] = \"Bayesian\"\n",
        "config[\"num_iteration\"] = 100\n",
        "\n",
        "tuner = Tuner(HYPERPARAMETERS, objective=run_one_training, conf_dict=config) \n",
        "results = tuner.minimize()"
      ],
      "metadata": {
        "id": "LiA-grd1P4ZG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}